{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b821212-eb82-4cd0-9a07-6371835cb161",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [Visual Information Theory](https://colah.github.io/posts/2015-09-Visual-Information/)\n",
    "\n",
    "## Optimal Encodings\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/code-costonly.png\" />\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/code-cost.png\" />\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/code-cost-longshort.png\" />\n",
    "\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/code-auction-balanced-noderivs.png\" />\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/code-auction-balanced.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e896d5b-e2a7-4814-8534-5a83a19ab85b",
   "metadata": {},
   "source": [
    "## Calculating Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142249ad-4203-4a1f-99fe-f16b45ff6492",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/entropy-def-notitle.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc37f6-9b91-448b-a78d-b7b53614dfe5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br />\n",
    "$$H(p)=∑xp(x)log2(1p(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e2246-d4ce-4e40-b7c2-42405dd1b7ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cross Entropy\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/DogCatWordFreq.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681d84b-b4d5-4dd6-82be-d8a628a22bd9",
   "metadata": {},
   "source": [
    "$$Hp(q)=∑xq(x)log2(1p(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950c587-38d1-4c78-a7d8-2fb4cfce9cb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/CrossEntropyDef.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae572b29-3aa8-45f6-b582-c88033119893",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/CrossEntropyCompare.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112cbb8-a8b1-41ed-844b-f749c623bf30",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Cross-entropy isn’t symmetric.**\n",
    "\n",
    "The more different `p` and `q` are the more the cross-entropy of `p` with respect to `q` will be bigger than the entropy of `p` and vice versa\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/CrossEntropyQP.png\" />\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/CrossEntropyPQ.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f6dbc-274e-4a7e-89ce-578acb38f4b1",
   "metadata": {},
   "source": [
    "The really interesting thing is the difference between the entropy and the cross-entropy. That difference is how much longer our messages are because we used a code optimized for a different distribution. If the distributions are the same, this difference will be zero. As the difference grows, it will get bigger.\n",
    "\n",
    "We call this difference the Kullback–Leibler divergence, or just the **KL divergence**. The KL divergence of `p` with respect to `q`, $Dq(p)$ is defined:\n",
    "\n",
    "$$Dq(p)=Hq(p)−H(p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca539892-6ae6-4367-a092-b39ca1bd251f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Entropy and Multiple Variables\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/prob-2D-factored1-detail.png\" />\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/prob-2D-factored1-flat.png\" />\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-flat.png\" />\n",
    "\n",
    "**Joint Entropy**\n",
    "\n",
    "$$H(X,Y)=∑x,yp(x,y)log2(1p(x,y))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398a304-cc9d-4896-a1c1-40565c33aafc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-3D.png\" style=\"height:400px;\" />\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/HxCy-sep.png\" style=\"height:400px;\" />\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/HxCy.png\" style=\"height:400px;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ce893-2e62-40e8-904d-3d12de60ff96",
   "metadata": {},
   "source": [
    "## Conditional Entropy\n",
    "\n",
    "$$H(X|Y) = \\sum_y p(y) \\sum_x p(x|y) \\log_2\\left(\\frac{1}{p(x|y)}\\right)$$\n",
    "$$~~~~ = \\sum_{x,y} p(x,y) \\log_2\\left(\\frac{1}{p(x|y)}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69df70-7b70-452b-852b-689c2df9a9d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mutual Information\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-info-1.png\" />\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-overview.png\" />\n",
    "\n",
    "For example, we previously noted it takes more information to communicate both X and Y (the “joint entropy,” H(X,Y)) than it takes to just communicate X (the “marginal entropy,” H(X)). But if you already know Y, then it takes less information to communicate X (the “conditional entropy,” H(X|Y)) than it would if you didn’t!\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-overview.png\" />\n",
    "\n",
    "**$$H(X,Y) \\geq H(X) \\geq H(X|Y)$$**\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-info-4.png\" />\n",
    "\n",
    "$$H(X,Y) = H(Y) + H(X|Y)$$\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-overview-sum.png\" />\n",
    "\n",
    "**Mutual Information**\n",
    "\n",
    "$$I(X,Y) = H(X) + H(Y) - H(X,Y)$$\n",
    "\n",
    "## Variation of Information\n",
    "$$V(X,Y) = H(X,Y) - I(X,Y)$$\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-info.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55222eb7-5762-40da-8e1c-ae10af85ead1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
